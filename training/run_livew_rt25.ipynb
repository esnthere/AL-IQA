{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_rt25.py:354: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0 = F.softmax(output_0[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_rt25.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1 = F.softmax(output_1[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_rt25.py:356: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2 = F.softmax(output_2[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_rt25.py:293: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0=F.softmax(output_0[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_rt25.py:294: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1=F.softmax(output_1[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_rt25.py:295: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2=F.softmax(output_2[:,:2])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Train ALL Pearson0: 0.18353062243301438\n",
      "Train  ALL Spearman0: 0.1810874090812713\n",
      "Train ALL Pearson1: 0.05042574516365413\n",
      "Train  ALL Spearman1: 0.10658288915712584\n",
      "Train ALL Pearson2: 0.01487548197802172\n",
      "Train  ALL Spearman2: 0.020135012720833696\n",
      "Train ALL Pearson0: 0.9395724604921398\n",
      "Train  ALL Spearman0: 0.9198304935276332\n",
      "Train ALL Pearson1: 0.8479668470150525\n",
      "Train  ALL Spearman1: 0.8464258591814996\n",
      "Train ALL Pearson2: 0.9496384196316686\n",
      "Train  ALL Spearman2: 0.9338204160234828\n",
      "Train ALL Pearson0: 0.9806813546701914\n",
      "Train  ALL Spearman0: 0.9724856446650895\n",
      "Train ALL Pearson1: 0.9484274669149454\n",
      "Train  ALL Spearman1: 0.9380132718637366\n",
      "Train ALL Pearson2: 0.9514584835108809\n",
      "Train  ALL Spearman2: 0.9375611919522221\n",
      "Train ALL Pearson0: 0.9810470508576691\n",
      "Train  ALL Spearman0: 0.97942483718138\n",
      "Train ALL Pearson1: 0.9626204807722896\n",
      "Train  ALL Spearman1: 0.9586006425061063\n",
      "Train ALL Pearson2: 0.9743566583638181\n",
      "Train  ALL Spearman2: 0.9677211978776825\n",
      "Train ALL Pearson0: 0.9797498584083116\n",
      "Train  ALL Spearman0: 0.9738568861534184\n",
      "Train ALL Pearson1: 0.9695871850677282\n",
      "Train  ALL Spearman1: 0.9614226883256901\n",
      "Train ALL Pearson2: 0.9691044432323052\n",
      "Train  ALL Spearman2: 0.9719863888480009\n",
      "Test ALL Pearson0: 0.5418368589345487 Test  ALL Spearman0: 0.509179350633634\n",
      "Test ALL Pearson1: 0.7408200582583759 Test  ALL Spearman1: 0.6909140389336218\n",
      "Test ALL Pearson2: 0.3919503463013531 Test  ALL Spearman2: 0.38044235260867626\n",
      "Weight: 0.005 Split: 0 End! PLCC: [0.5582024211647593] SRCC: [0.5268452473919772]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.23938937959448908\n",
      "Train  ALL Spearman0: 0.3175939379513121\n",
      "Train ALL Pearson1: 0.0932956429436088\n",
      "Train  ALL Spearman1: 0.09265484074329418\n",
      "Train ALL Pearson2: 0.31808434952158543\n",
      "Train  ALL Spearman2: 0.3653818055857528\n",
      "Train ALL Pearson0: 0.9541158613666179\n",
      "Train  ALL Spearman0: 0.9547301085881277\n",
      "Train ALL Pearson1: 0.9194339490663362\n",
      "Train  ALL Spearman1: 0.8836604477819201\n",
      "Train ALL Pearson2: 0.9337278871279763\n",
      "Train  ALL Spearman2: 0.9223327974837935\n",
      "Train ALL Pearson0: 0.9286705725072247\n",
      "Train  ALL Spearman0: 0.9170101909371702\n",
      "Train ALL Pearson1: 0.86367332813814\n",
      "Train  ALL Spearman1: 0.8342880580741129\n",
      "Train ALL Pearson2: 0.9214398753544271\n",
      "Train  ALL Spearman2: 0.9206377088480935\n",
      "Test ALL Pearson0: 0.4500006432471513 Test  ALL Spearman0: 0.46370063050871363\n",
      "Test ALL Pearson1: 0.6087763770580074 Test  ALL Spearman1: 0.5692124462243532\n",
      "Test ALL Pearson2: 0.7330905572068859 Test  ALL Spearman2: 0.6932465903092693\n",
      "Weight: 0.005 Split: 1 End! PLCC: [0.5582024211647593, 0.5972891925040148] SRCC: [0.5268452473919772, 0.5753865556807787]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.19358699049460076\n",
      "Train  ALL Spearman0: -0.1525198359078974\n",
      "Train ALL Pearson1: -0.31141142896514395\n",
      "Train  ALL Spearman1: -0.3628844898996522\n",
      "Train ALL Pearson2: 0.28570609246440637\n",
      "Train  ALL Spearman2: 0.27411317449933603\n",
      "Train ALL Pearson0: 0.95055707316425\n",
      "Train  ALL Spearman0: 0.9458732821239609\n",
      "Train ALL Pearson1: 0.9287988823708837\n",
      "Train  ALL Spearman1: 0.925320803864734\n",
      "Train ALL Pearson2: 0.9258287687494325\n",
      "Train  ALL Spearman2: 0.9105415471768004\n",
      "Train ALL Pearson0: 0.965477446092303\n",
      "Train  ALL Spearman0: 0.9642878033889017\n",
      "Train ALL Pearson1: 0.9583362804754483\n",
      "Train  ALL Spearman1: 0.9457856038441942\n",
      "Train ALL Pearson2: 0.9537326243164198\n",
      "Train  ALL Spearman2: 0.9530170080315233\n",
      "Test ALL Pearson0: 0.5953368310722104 Test  ALL Spearman0: 0.5594732434071025\n",
      "Test ALL Pearson1: 0.49152312289726097 Test  ALL Spearman1: 0.462166694006331\n",
      "Test ALL Pearson2: 0.7095685404784152 Test  ALL Spearman2: 0.6761426542485582\n",
      "Weight: 0.005 Split: 2 End! PLCC: [0.5582024211647593, 0.5972891925040148, 0.5988094981492956] SRCC: [0.5268452473919772, 0.5753865556807787, 0.5659275305539972]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.04165632839204426\n",
      "Train  ALL Spearman0: 0.027181855798807105\n",
      "Train ALL Pearson1: -0.06760608166132685\n",
      "Train  ALL Spearman1: -0.008650544335209554\n",
      "Train ALL Pearson2: 0.3744614483696921\n",
      "Train  ALL Spearman2: 0.47594833210060783\n",
      "Train ALL Pearson0: 0.8023345970598933\n",
      "Train  ALL Spearman0: 0.83008888040509\n",
      "Train ALL Pearson1: 0.9081493914086184\n",
      "Train  ALL Spearman1: 0.9038565637248093\n",
      "Train ALL Pearson2: 0.8708791927089519\n",
      "Train  ALL Spearman2: 0.884582794773365\n",
      "Train ALL Pearson0: 0.7864469578337506\n",
      "Train  ALL Spearman0: 0.8565624834898906\n",
      "Train ALL Pearson1: 0.8861001903620879\n",
      "Train  ALL Spearman1: 0.8646326509596328\n",
      "Train ALL Pearson2: 0.8340562773428161\n",
      "Train  ALL Spearman2: 0.8232818438832946\n",
      "Test ALL Pearson0: 0.3083200514444991 Test  ALL Spearman0: 0.3027527819650881\n",
      "Test ALL Pearson1: 0.6479567728787746 Test  ALL Spearman1: 0.6193470589720322\n",
      "Test ALL Pearson2: 0.7183760711945677 Test  ALL Spearman2: 0.6959754067881393\n",
      "Weight: 0.01 Split: 0 End! PLCC: [0.5582176318392805] SRCC: [0.5393584159084198]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson0: 0.08165472021548753\n",
      "Train  ALL Spearman0: 0.05108571782209035\n",
      "Train ALL Pearson1: -0.042460381199023796\n",
      "Train  ALL Spearman1: 0.009147900884307792\n",
      "Train ALL Pearson2: 0.5328555178996959\n",
      "Train  ALL Spearman2: 0.5823591342109148\n",
      "Train ALL Pearson0: 0.9157341685129949\n",
      "Train  ALL Spearman0: 0.8997092039058501\n",
      "Train ALL Pearson1: 0.940137034042762\n",
      "Train  ALL Spearman1: 0.9126724401856091\n",
      "Train ALL Pearson2: 0.9549830321446171\n",
      "Train  ALL Spearman2: 0.9445240311102687\n",
      "Train ALL Pearson0: 0.9212270255603451\n",
      "Train  ALL Spearman0: 0.9221282178208641\n",
      "Train ALL Pearson1: 0.9469817293345888\n",
      "Train  ALL Spearman1: 0.9252305048561088\n",
      "Train ALL Pearson2: 0.9491601806029331\n",
      "Train  ALL Spearman2: 0.9391393317971589\n",
      "Test ALL Pearson0: 0.6695518089587784 Test  ALL Spearman0: 0.6329236358789265\n",
      "Test ALL Pearson1: 0.5997582346920448 Test  ALL Spearman1: 0.5628701109871077\n",
      "Test ALL Pearson2: 0.7578577685331394 Test  ALL Spearman2: 0.7306214053756457\n",
      "Weight: 0.01 Split: 1 End! PLCC: [0.5582176318392805, 0.6757226040613209] SRCC: [0.5393584159084198, 0.64213838408056]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.030395415376085005\n",
      "Train  ALL Spearman0: 0.019991232129286385\n",
      "Train ALL Pearson1: -0.035879258122120804\n",
      "Train  ALL Spearman1: -0.042404582455973346\n",
      "Train ALL Pearson2: 0.38917611367029664\n",
      "Train  ALL Spearman2: 0.3675181226890622\n",
      "Train ALL Pearson0: 0.9469992842852735\n",
      "Train  ALL Spearman0: 0.9351901996679897\n",
      "Train ALL Pearson1: 0.9324080698303154\n",
      "Train  ALL Spearman1: 0.8938092141944168\n",
      "Train ALL Pearson2: 0.8114914941537714\n",
      "Train  ALL Spearman2: 0.8016014498294168\n",
      "Train ALL Pearson0: 0.9259201543490315\n",
      "Train  ALL Spearman0: 0.9025092597147628\n",
      "Train ALL Pearson1: 0.9186243822492436\n",
      "Train  ALL Spearman1: 0.9004719992164885\n",
      "Train ALL Pearson2: 0.7901512624659816\n",
      "Train  ALL Spearman2: 0.8001285941265387\n",
      "Test ALL Pearson0: 0.45338263751432945 Test  ALL Spearman0: 0.44024933376705727\n",
      "Test ALL Pearson1: 0.7656917064672224 Test  ALL Spearman1: 0.7325205867981216\n",
      "Test ALL Pearson2: 0.7368438253659609 Test  ALL Spearman2: 0.724390486834073\n",
      "Weight: 0.01 Split: 2 End! PLCC: [0.5582176318392805, 0.6757226040613209, 0.6519727231158376] SRCC: [0.5393584159084198, 0.64213838408056, 0.6323868024664173]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.04652682285094448\n",
      "Train  ALL Spearman0: -0.05001242426697731\n",
      "Train ALL Pearson1: -0.05799059782278546\n",
      "Train  ALL Spearman1: -0.009060751036711463\n",
      "Train ALL Pearson2: 0.6264382348312453\n",
      "Train  ALL Spearman2: 0.6328325972633213\n",
      "Train ALL Pearson0: 0.945171138120046\n",
      "Train  ALL Spearman0: 0.9283992114875619\n",
      "Train ALL Pearson1: 0.9311209787267511\n",
      "Train  ALL Spearman1: 0.9253181767295949\n",
      "Train ALL Pearson2: 0.9556831893579582\n",
      "Train  ALL Spearman2: 0.9530462354062598\n",
      "Train ALL Pearson0: 0.9753884711172094\n",
      "Train  ALL Spearman0: 0.973317360610353\n",
      "Train ALL Pearson1: 0.9380327357902498\n",
      "Train  ALL Spearman1: 0.9266717388550375\n",
      "Train ALL Pearson2: 0.9674398971927802\n",
      "Train  ALL Spearman2: 0.9672385108923927\n",
      "Test ALL Pearson0: 0.647970713028253 Test  ALL Spearman0: 0.6111670239519374\n",
      "Test ALL Pearson1: 0.643132430891511 Test  ALL Spearman1: 0.6006871481205323\n",
      "Test ALL Pearson2: 0.7488681250760305 Test  ALL Spearman2: 0.7074196011942048\n",
      "Weight: 0.1 Split: 0 End! PLCC: [0.6799904229985981] SRCC: [0.6397579244222249]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.176764974892222\n",
      "Train  ALL Spearman0: 0.25035442071692615\n",
      "Train ALL Pearson1: 0.07747441173773736\n",
      "Train  ALL Spearman1: 0.11672731053424604\n",
      "Train ALL Pearson2: 0.4076212984129492\n",
      "Train  ALL Spearman2: 0.45029958101742007\n",
      "Train ALL Pearson0: 0.9500885326981352\n",
      "Train  ALL Spearman0: 0.9517031836923792\n",
      "Train ALL Pearson1: 0.9348919555792379\n",
      "Train  ALL Spearman1: 0.9323455507187579\n",
      "Train ALL Pearson2: 0.9389437756593539\n",
      "Train  ALL Spearman2: 0.928852197993126\n",
      "Train ALL Pearson0: 0.975634395442592\n",
      "Train  ALL Spearman0: 0.9695898141754346\n",
      "Train ALL Pearson1: 0.955978495793231\n",
      "Train  ALL Spearman1: 0.951399849658313\n",
      "Train ALL Pearson2: 0.9699432068981548\n",
      "Train  ALL Spearman2: 0.970890817950747\n",
      "Test ALL Pearson0: 0.470012679097573 Test  ALL Spearman0: 0.4469778218742675\n",
      "Test ALL Pearson1: 0.7163362629286515 Test  ALL Spearman1: 0.6727168267256887\n",
      "Test ALL Pearson2: 0.6973544698423986 Test  ALL Spearman2: 0.6669063800522099\n",
      "Weight: 0.1 Split: 1 End! PLCC: [0.6799904229985981, 0.627901137289541] SRCC: [0.6397579244222249, 0.5955336762173887]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.37743766461695943\n",
      "Train  ALL Spearman0: 0.3375279649584987\n",
      "Train ALL Pearson1: 0.018657453812736997\n",
      "Train  ALL Spearman1: 0.07145884237103052\n",
      "Train ALL Pearson2: 0.48258700566085416\n",
      "Train  ALL Spearman2: 0.5606067277586149\n",
      "Train ALL Pearson0: 0.9186680367319606\n",
      "Train  ALL Spearman0: 0.9104246462206856\n",
      "Train ALL Pearson1: 0.9512763045040622\n",
      "Train  ALL Spearman1: 0.9510865765631966\n",
      "Train ALL Pearson2: 0.969912321510348\n",
      "Train  ALL Spearman2: 0.968682880890262\n",
      "Train ALL Pearson0: 0.8719024758207579\n",
      "Train  ALL Spearman0: 0.8798643965561856\n",
      "Train ALL Pearson1: 0.9169439190162434\n",
      "Train  ALL Spearman1: 0.9090962251194717\n",
      "Train ALL Pearson2: 0.9386444773665089\n",
      "Train  ALL Spearman2: 0.9265748069162284\n",
      "Test ALL Pearson0: 0.22330026597315225 Test  ALL Spearman0: 0.22074081624408787\n",
      "Test ALL Pearson1: 0.7248496599443175 Test  ALL Spearman1: 0.6657836034491911\n",
      "Test ALL Pearson2: 0.7261177721534943 Test  ALL Spearman2: 0.6936225156965424\n",
      "Weight: 0.1 Split: 2 End! PLCC: [0.6799904229985981, 0.627901137289541, 0.5580892326903214] SRCC: [0.6397579244222249, 0.5955336762173887, 0.5267156451299404]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.20704739293669674\n",
      "Train  ALL Spearman0: -0.20895566450277922\n",
      "Train ALL Pearson1: -0.22720046236605038\n",
      "Train  ALL Spearman1: -0.2894006255071351\n",
      "Train ALL Pearson2: 0.2664381172353867\n",
      "Train  ALL Spearman2: 0.23962473860259412\n",
      "Train ALL Pearson0: 0.9474735780848088\n",
      "Train  ALL Spearman0: 0.9431017859604425\n",
      "Train ALL Pearson1: 0.951628370811908\n",
      "Train  ALL Spearman1: 0.9537503885765112\n",
      "Train ALL Pearson2: 0.964919916509718\n",
      "Train  ALL Spearman2: 0.9616986536682042\n",
      "Train ALL Pearson0: 0.9512654928340051\n",
      "Train  ALL Spearman0: 0.9443859972879247\n",
      "Train ALL Pearson1: 0.9600677573213658\n",
      "Train  ALL Spearman1: 0.9515459715826962\n",
      "Train ALL Pearson2: 0.9678631477142884\n",
      "Train  ALL Spearman2: 0.9588088324012658\n",
      "Test ALL Pearson0: 0.7179743809133118 Test  ALL Spearman0: 0.6920539753316239\n",
      "Test ALL Pearson1: 0.6415936084620875 Test  ALL Spearman1: 0.6156672931177736\n",
      "Test ALL Pearson2: 0.7339610667134425 Test  ALL Spearman2: 0.7052436272637821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 1 Split: 0 End! PLCC: [0.6978430186962806] SRCC: [0.6709882985710598]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.079930254494226\n",
      "Train  ALL Spearman0: -0.11177528369194695\n",
      "Train ALL Pearson1: -0.21776887543557635\n",
      "Train  ALL Spearman1: -0.18457896507694094\n",
      "Train ALL Pearson2: 0.3456905596646067\n",
      "Train  ALL Spearman2: 0.31789221634106796\n",
      "Train ALL Pearson0: 0.9030840942946605\n",
      "Train  ALL Spearman0: 0.8987900458892032\n",
      "Train ALL Pearson1: 0.931968097873277\n",
      "Train  ALL Spearman1: 0.9252981124720246\n",
      "Train ALL Pearson2: 0.9467435899162835\n",
      "Train  ALL Spearman2: 0.9329008140314136\n",
      "Train ALL Pearson0: 0.940645341980546\n",
      "Train  ALL Spearman0: 0.9211503089442024\n",
      "Train ALL Pearson1: 0.9321005688836844\n",
      "Train  ALL Spearman1: 0.9196756070429375\n",
      "Train ALL Pearson2: 0.94551934988988\n",
      "Train  ALL Spearman2: 0.9396379160656497\n",
      "Test ALL Pearson0: 0.5031779534438896 Test  ALL Spearman0: 0.5027526648194391\n",
      "Test ALL Pearson1: 0.7348160892264378 Test  ALL Spearman1: 0.7027691583491575\n",
      "Test ALL Pearson2: 0.7361206753335783 Test  ALL Spearman2: 0.7116131853856905\n",
      "Weight: 1 Split: 1 End! PLCC: [0.6978430186962806, 0.6580382393346352] SRCC: [0.6709882985710598, 0.6390450028514291]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.20704739293669674\n",
      "Train  ALL Spearman0: -0.20895566450277922\n",
      "Train ALL Pearson1: -0.22720046236605038\n",
      "Train  ALL Spearman1: -0.2894006255071351\n",
      "Train ALL Pearson2: 0.2664381172353867\n",
      "Train  ALL Spearman2: 0.23962473860259412\n",
      "Train ALL Pearson0: 0.9474735780848088\n",
      "Train  ALL Spearman0: 0.9431017859604425\n",
      "Train ALL Pearson1: 0.951628370811908\n",
      "Train  ALL Spearman1: 0.9537503885765112\n",
      "Train ALL Pearson2: 0.964919916509718\n",
      "Train  ALL Spearman2: 0.9616986536682042\n",
      "Train ALL Pearson0: 0.9512654928340051\n",
      "Train  ALL Spearman0: 0.9443859972879247\n",
      "Train ALL Pearson1: 0.9600677573213658\n",
      "Train  ALL Spearman1: 0.9515459715826962\n",
      "Train ALL Pearson2: 0.9678631477142884\n",
      "Train  ALL Spearman2: 0.9588088324012658\n",
      "Test ALL Pearson0: 0.7179743809133118 Test  ALL Spearman0: 0.6920539753316239\n",
      "Test ALL Pearson1: 0.6415936084620875 Test  ALL Spearman1: 0.6156672931177736\n",
      "Test ALL Pearson2: 0.7339610667134425 Test  ALL Spearman2: 0.7052436272637821\n",
      "Weight: 1 Split: 2 End! PLCC: [0.6978430186962806, 0.6580382393346352, 0.6978430186962806] SRCC: [0.6709882985710598, 0.6390450028514291, 0.6709882985710598]\n"
     ]
    }
   ],
   "source": [
    "run livew_rt25_first_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_second_step_rt25.py:354: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0 = F.softmax(output_0[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_second_step_rt25.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1 = F.softmax(output_1[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_second_step_rt25.py:356: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2 = F.softmax(output_2[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_second_step_rt25.py:293: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0=F.softmax(output_0[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_second_step_rt25.py:294: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1=F.softmax(output_1[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_second_step_rt25.py:295: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2=F.softmax(output_2[:,:2])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Train ALL Pearson0: 0.10040508692781548\n",
      "Train  ALL Spearman0: 0.12786139405714614\n",
      "Train ALL Pearson1: -0.06811854421657727\n",
      "Train  ALL Spearman1: -0.06257231932033101\n",
      "Train ALL Pearson2: 0.4221596594401117\n",
      "Train  ALL Spearman2: 0.4272197948081306\n",
      "Train ALL Pearson0: 0.9027751101725\n",
      "Train  ALL Spearman0: 0.8826328147254678\n",
      "Train ALL Pearson1: 0.906881233195602\n",
      "Train  ALL Spearman1: 0.8754572916252025\n",
      "Train ALL Pearson2: 0.9132190203330992\n",
      "Train  ALL Spearman2: 0.9059748205777035\n",
      "Train ALL Pearson0: 0.9374666019524275\n",
      "Train  ALL Spearman0: 0.9261345293931585\n",
      "Train ALL Pearson1: 0.9291327509434663\n",
      "Train  ALL Spearman1: 0.9079818764736923\n",
      "Train ALL Pearson2: 0.9304306342209798\n",
      "Train  ALL Spearman2: 0.9272700210901046\n",
      "Test ALL Pearson0: 0.700004743218485 Test  ALL Spearman0: 0.6764293903266624\n",
      "Test ALL Pearson1: 0.801054438384993 Test  ALL Spearman1: 0.7701720620037483\n",
      "Test ALL Pearson2: 0.7732342422725363 Test  ALL Spearman2: 0.7452373023078968\n",
      "Weight: 0.005 Split: 0 End! PLCC: [0.7580978079586714] SRCC: [0.7306129182127692]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.15995320816619513\n",
      "Train  ALL Spearman0: 0.15122806141918008\n",
      "Train ALL Pearson1: -0.03730162844362227\n",
      "Train  ALL Spearman1: -0.022374139874724246\n",
      "Train ALL Pearson2: 0.2644070870619046\n",
      "Train  ALL Spearman2: 0.27138480267319465\n",
      "Train ALL Pearson0: 0.7907007722516365\n",
      "Train  ALL Spearman0: 0.7663236686014346\n",
      "Train ALL Pearson1: 0.9025637177945176\n",
      "Train  ALL Spearman1: 0.8924497273923472\n",
      "Train ALL Pearson2: 0.8991194836893138\n",
      "Train  ALL Spearman2: 0.8926844838605811\n",
      "Test ALL Pearson0: 0.601326571166501 Test  ALL Spearman0: 0.5921661774991364\n",
      "Test ALL Pearson1: 0.740389191354981 Test  ALL Spearman1: 0.732676849207102\n",
      "Test ALL Pearson2: 0.7792406352517847 Test  ALL Spearman2: 0.7520971542866693\n",
      "Weight: 0.005 Split: 1 End! PLCC: [0.7580978079586714, 0.7069854659244222] SRCC: [0.7306129182127692, 0.6923133936643026]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.10974409709889169\n",
      "Train  ALL Spearman0: 0.0928405878790758\n",
      "Train ALL Pearson1: -0.10020683388996174\n",
      "Train  ALL Spearman1: -0.08059816908656144\n",
      "Train ALL Pearson2: 0.52934864196666\n",
      "Train  ALL Spearman2: 0.5216023420090147\n",
      "Train ALL Pearson0: 0.8326897289570987\n",
      "Train  ALL Spearman0: 0.8143031019300182\n",
      "Train ALL Pearson1: 0.9290185686239104\n",
      "Train  ALL Spearman1: 0.9063794593639902\n",
      "Train ALL Pearson2: 0.9113348415641651\n",
      "Train  ALL Spearman2: 0.9076668587346012\n",
      "Test ALL Pearson0: 0.608068928959561 Test  ALL Spearman0: 0.5889902422350068\n",
      "Test ALL Pearson1: 0.826056063077519 Test  ALL Spearman1: 0.7862709383820987\n",
      "Test ALL Pearson2: 0.7587664154574514 Test  ALL Spearman2: 0.7433582038577967\n",
      "Weight: 0.005 Split: 2 End! PLCC: [0.7580978079586714, 0.7069854659244222, 0.7309638024981772] SRCC: [0.7306129182127692, 0.6923133936643026, 0.7062064614916341]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.02679248981976553\n",
      "Train  ALL Spearman0: 0.015203275454786407\n",
      "Train ALL Pearson1: 0.06241277216567221\n",
      "Train  ALL Spearman1: 0.02646363996757202\n",
      "Train ALL Pearson2: 0.42416131876149366\n",
      "Train  ALL Spearman2: 0.37298140068982116\n",
      "Train ALL Pearson0: 0.8925591739553952\n",
      "Train  ALL Spearman0: 0.8677108705842029\n",
      "Train ALL Pearson1: 0.8668608481298067\n",
      "Train  ALL Spearman1: 0.8378076544131265\n",
      "Train ALL Pearson2: 0.8488931702160368\n",
      "Train  ALL Spearman2: 0.8488640863339336\n",
      "Test ALL Pearson0: 0.6300946216937502 Test  ALL Spearman0: 0.6262378206949043\n",
      "Test ALL Pearson1: 0.8343367886444782 Test  ALL Spearman1: 0.7984990234961592\n",
      "Test ALL Pearson2: 0.8071303010277123 Test  ALL Spearman2: 0.7793736253453227\n",
      "Weight: 0.01 Split: 0 End! PLCC: [0.7571872371219803] SRCC: [0.7347034898454621]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.09595588259771147\n",
      "Train  ALL Spearman0: 0.10785809217814311\n",
      "Train ALL Pearson1: -0.003945256158642661\n",
      "Train  ALL Spearman1: -0.02873578662632921\n",
      "Train ALL Pearson2: 0.37512669883778077\n",
      "Train  ALL Spearman2: 0.4494832504021751\n",
      "Train ALL Pearson0: 0.8730281241095966\n",
      "Train  ALL Spearman0: 0.854966715358211\n",
      "Train ALL Pearson1: 0.8739174125201337\n",
      "Train  ALL Spearman1: 0.8629834621151012\n",
      "Train ALL Pearson2: 0.86441344021624\n",
      "Train  ALL Spearman2: 0.847956452086214\n",
      "Test ALL Pearson0: 0.5811829341096378 Test  ALL Spearman0: 0.5700702792772724\n",
      "Test ALL Pearson1: 0.7906837421123759 Test  ALL Spearman1: 0.773309253473349\n",
      "Test ALL Pearson2: 0.8218708107329333 Test  ALL Spearman2: 0.7960359112950214\n",
      "Weight: 0.01 Split: 1 End! PLCC: [0.7571872371219803, 0.7312458289849824] SRCC: [0.7347034898454621, 0.7131384813485476]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.12277197604000284\n",
      "Train  ALL Spearman0: 0.1442992274324234\n",
      "Train ALL Pearson1: -0.0037016638592136633\n",
      "Train  ALL Spearman1: 0.005017189572128438\n",
      "Train ALL Pearson2: 0.6629819510044949\n",
      "Train  ALL Spearman2: 0.6643374969413026\n",
      "Train ALL Pearson0: 0.9303758966941521\n",
      "Train  ALL Spearman0: 0.9176852745405235\n",
      "Train ALL Pearson1: 0.914951851055851\n",
      "Train  ALL Spearman1: 0.8859064768946003\n",
      "Train ALL Pearson2: 0.9206485716067754\n",
      "Train  ALL Spearman2: 0.9166402164497807\n",
      "Test ALL Pearson0: 0.7639134276602353 Test  ALL Spearman0: 0.7352366794239042\n",
      "Test ALL Pearson1: 0.7973741213003006 Test  ALL Spearman1: 0.7600825421527286\n",
      "Test ALL Pearson2: 0.7873062290995875 Test  ALL Spearman2: 0.7628340241450924\n",
      "Weight: 0.01 Split: 2 End! PLCC: [0.7571872371219803, 0.7312458289849824, 0.7828645926867077] SRCC: [0.7347034898454621, 0.7131384813485476, 0.7527177485739084]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.20018864701160305\n",
      "Train  ALL Spearman0: 0.19806113981498671\n",
      "Train ALL Pearson1: -0.1624634008697051\n",
      "Train  ALL Spearman1: -0.09696420078251787\n",
      "Train ALL Pearson2: 0.3689560311727036\n",
      "Train  ALL Spearman2: 0.3887098730130632\n",
      "Train ALL Pearson0: 0.8795677676105953\n",
      "Train  ALL Spearman0: 0.850311430845786\n",
      "Train ALL Pearson1: 0.8763308817240744\n",
      "Train  ALL Spearman1: 0.8505548819892949\n",
      "Train ALL Pearson2: 0.9148211759552637\n",
      "Train  ALL Spearman2: 0.9023556138165022\n",
      "Test ALL Pearson0: 0.6391535412979428 Test  ALL Spearman0: 0.6161457313998819\n",
      "Test ALL Pearson1: 0.7962444886146313 Test  ALL Spearman1: 0.772524249999651\n",
      "Test ALL Pearson2: 0.7866057634739525 Test  ALL Spearman2: 0.7661321290994055\n",
      "Weight: 0.1 Split: 0 End! PLCC: [0.7406679311288421] SRCC: [0.7182673701663128]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.1250230738591301\n",
      "Train  ALL Spearman0: 0.15167551493243508\n",
      "Train ALL Pearson1: -0.2762084073533594\n",
      "Train  ALL Spearman1: -0.23163576467918595\n",
      "Train ALL Pearson2: 0.5328412153101579\n",
      "Train  ALL Spearman2: 0.5424990211714154\n",
      "Train ALL Pearson0: 0.8824355487866286\n",
      "Train  ALL Spearman0: 0.8672122990780893\n",
      "Train ALL Pearson1: 0.8759070194937854\n",
      "Train  ALL Spearman1: 0.8552489134990144\n",
      "Train ALL Pearson2: 0.8852747343973676\n",
      "Train  ALL Spearman2: 0.8759328571788382\n",
      "Test ALL Pearson0: 0.652113534635535 Test  ALL Spearman0: 0.6301866551376343\n",
      "Test ALL Pearson1: 0.8283244380814457 Test  ALL Spearman1: 0.7993750097170087\n",
      "Test ALL Pearson2: 0.7722602712517032 Test  ALL Spearman2: 0.7469660525547641\n",
      "Weight: 0.1 Split: 1 End! PLCC: [0.7406679311288421, 0.750899414656228] SRCC: [0.7182673701663128, 0.725509239136469]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.08519284379305712\n",
      "Train  ALL Spearman0: -0.09575257924869937\n",
      "Train ALL Pearson1: -0.07359970430931514\n",
      "Train  ALL Spearman1: -0.06905628423779925\n",
      "Train ALL Pearson2: 0.44419435908722527\n",
      "Train  ALL Spearman2: 0.42606479335226066\n",
      "Train ALL Pearson0: 0.8496698812727063\n",
      "Train  ALL Spearman0: 0.8492721100981944\n",
      "Train ALL Pearson1: 0.9240378235243834\n",
      "Train  ALL Spearman1: 0.9076008780494468\n",
      "Train ALL Pearson2: 0.9112098077900912\n",
      "Train  ALL Spearman2: 0.904367882323022\n",
      "Test ALL Pearson0: 0.6354555845754457 Test  ALL Spearman0: 0.6124485993054403\n",
      "Test ALL Pearson1: 0.8195738308683889 Test  ALL Spearman1: 0.7917111457276981\n",
      "Test ALL Pearson2: 0.7795640703509384 Test  ALL Spearman2: 0.7545452018444275\n",
      "Weight: 0.1 Split: 2 End! PLCC: [0.7406679311288421, 0.750899414656228, 0.7448644952649244] SRCC: [0.7182673701663128, 0.725509239136469, 0.7195683156258553]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.06692778648488351\n",
      "Train  ALL Spearman0: 0.07069756520471787\n",
      "Train ALL Pearson1: 0.1685814168230039\n",
      "Train  ALL Spearman1: 0.17581925467063853\n",
      "Train ALL Pearson2: 0.09684228123551011\n",
      "Train  ALL Spearman2: 0.11549319592769791\n",
      "Train ALL Pearson0: 0.9264870652726325\n",
      "Train  ALL Spearman0: 0.9223383366069716\n",
      "Train ALL Pearson1: 0.9398089121706342\n",
      "Train  ALL Spearman1: 0.9321415173115329\n",
      "Train ALL Pearson2: 0.9367661291962612\n",
      "Train  ALL Spearman2: 0.929390176515801\n",
      "Test ALL Pearson0: 0.627511390948343 Test  ALL Spearman0: 0.5997161393841287\n",
      "Test ALL Pearson1: 0.7978042116649033 Test  ALL Spearman1: 0.7504481623246239\n",
      "Test ALL Pearson2: 0.7966979323355355 Test  ALL Spearman2: 0.754226533213955\n",
      "Weight: 1 Split: 0 End! PLCC: [0.7406711783162606] SRCC: [0.7014636116409024]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.036663579330090554\n",
      "Train  ALL Spearman0: -0.06194110651186346\n",
      "Train ALL Pearson1: 0.2239605615224567\n",
      "Train  ALL Spearman1: 0.23898185281522508\n",
      "Train ALL Pearson2: 0.4455228232794371\n",
      "Train  ALL Spearman2: 0.45453573469361036\n",
      "Train ALL Pearson0: 0.8852742456717039\n",
      "Train  ALL Spearman0: 0.879437150721882\n",
      "Train ALL Pearson1: 0.9177812766436072\n",
      "Train  ALL Spearman1: 0.9089018992922547\n",
      "Train ALL Pearson2: 0.9171519891879579\n",
      "Train  ALL Spearman2: 0.9102244877947674\n",
      "Test ALL Pearson0: 0.6321510284705912 Test  ALL Spearman0: 0.5878317977395425\n",
      "Test ALL Pearson1: 0.7934898913164609 Test  ALL Spearman1: 0.7514543042494527\n",
      "Test ALL Pearson2: 0.789089922456299 Test  ALL Spearman2: 0.7501634730837877\n",
      "Weight: 1 Split: 1 End! PLCC: [0.7406711783162606, 0.7382436140811169] SRCC: [0.7014636116409024, 0.6964831916909276]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.07418776980602924\n",
      "Train  ALL Spearman0: 0.07446927047788837\n",
      "Train ALL Pearson1: -0.32636641433727154\n",
      "Train  ALL Spearman1: -0.2970749315898804\n",
      "Train ALL Pearson2: 0.5967734436894598\n",
      "Train  ALL Spearman2: 0.6182140845663081\n",
      "Train ALL Pearson0: 0.9314947430221611\n",
      "Train  ALL Spearman0: 0.9223192928731092\n",
      "Train ALL Pearson1: 0.9142898968861581\n",
      "Train  ALL Spearman1: 0.9140680599597975\n",
      "Train ALL Pearson2: 0.919615079356698\n",
      "Train  ALL Spearman2: 0.9078750116758305\n",
      "Train ALL Pearson0: 0.9471324834086636\n",
      "Train  ALL Spearman0: 0.9411312257304957\n",
      "Train ALL Pearson1: 0.9464526740275184\n",
      "Train  ALL Spearman1: 0.937716680022717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson2: 0.9383985016697874\n",
      "Train  ALL Spearman2: 0.9254813364562977\n",
      "Test ALL Pearson0: 0.7512166612075417 Test  ALL Spearman0: 0.7117907480602959\n",
      "Test ALL Pearson1: 0.6575273712058224 Test  ALL Spearman1: 0.6089844810940818\n",
      "Test ALL Pearson2: 0.7967064040688807 Test  ALL Spearman2: 0.7567904333578216\n",
      "Weight: 1 Split: 2 End! PLCC: [0.7406711783162606, 0.7382436140811169, 0.7351501454940816] SRCC: [0.7014636116409024, 0.6964831916909276, 0.6925218875040664]\n"
     ]
    }
   ],
   "source": [
    "run livew_rt25_second_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25.py:354: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0 = F.softmax(output_0[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1 = F.softmax(output_1[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25.py:356: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2 = F.softmax(output_2[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25.py:293: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0=F.softmax(output_0[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25.py:294: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1=F.softmax(output_1[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25.py:295: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2=F.softmax(output_2[:,:2])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Train ALL Pearson0: -0.011171418424992884\n",
      "Train  ALL Spearman0: 0.006764038374334792\n",
      "Train ALL Pearson1: 0.32144965518384067\n",
      "Train  ALL Spearman1: 0.34064434710711106\n",
      "Train ALL Pearson2: 0.562667408763339\n",
      "Train  ALL Spearman2: 0.5680067157377918\n",
      "Train ALL Pearson0: 0.912891194446136\n",
      "Train  ALL Spearman0: 0.9024247745501902\n",
      "Train ALL Pearson1: 0.9091070719422644\n",
      "Train  ALL Spearman1: 0.8959360021564546\n",
      "Train ALL Pearson2: 0.9287458893498347\n",
      "Train  ALL Spearman2: 0.9236095720595143\n",
      "Test ALL Pearson0: 0.8281256475649481 Test  ALL Spearman0: 0.7910072874914311\n",
      "Test ALL Pearson1: 0.8350585765966393 Test  ALL Spearman1: 0.7951733840452289\n",
      "Test ALL Pearson2: 0.8157043453867688 Test  ALL Spearman2: 0.7806491796295794\n",
      "Weight: 0.005 Split: 0 End! PLCC: [0.826296189849452] SRCC: [0.7889432837220798]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.03758275502854241\n",
      "Train  ALL Spearman0: -0.09582725813279092\n",
      "Train ALL Pearson1: -0.01857162108606818\n",
      "Train  ALL Spearman1: -0.031190450803724425\n",
      "Train ALL Pearson2: 0.5109970061470587\n",
      "Train  ALL Spearman2: 0.517602247714734\n",
      "Train ALL Pearson0: 0.8949434307007432\n",
      "Train  ALL Spearman0: 0.878718967335685\n",
      "Train ALL Pearson1: 0.8989408723495981\n",
      "Train  ALL Spearman1: 0.8866910566786933\n",
      "Train ALL Pearson2: 0.8962734862361973\n",
      "Train  ALL Spearman2: 0.8839552774356042\n",
      "Test ALL Pearson0: 0.7702671205982331 Test  ALL Spearman0: 0.7235848858141162\n",
      "Test ALL Pearson1: 0.7953112490977351 Test  ALL Spearman1: 0.7539626306873236\n",
      "Test ALL Pearson2: 0.8414149197164904 Test  ALL Spearman2: 0.8075100437234045\n",
      "Weight: 0.005 Split: 1 End! PLCC: [0.826296189849452, 0.8023310964708195] SRCC: [0.7889432837220798, 0.7616858534082814]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.06798549178555247\n",
      "Train  ALL Spearman0: 0.06488297762842717\n",
      "Train ALL Pearson1: 0.02129147530406273\n",
      "Train  ALL Spearman1: 0.0574382054927857\n",
      "Train ALL Pearson2: 0.4543612904743906\n",
      "Train  ALL Spearman2: 0.46704461596370167\n",
      "Train ALL Pearson0: 0.9073490732050918\n",
      "Train  ALL Spearman0: 0.8871322180508855\n",
      "Train ALL Pearson1: 0.9046182704460198\n",
      "Train  ALL Spearman1: 0.8927988354265814\n",
      "Train ALL Pearson2: 0.9085892985223868\n",
      "Train  ALL Spearman2: 0.9061616771766899\n",
      "Test ALL Pearson0: 0.8063445717804788 Test  ALL Spearman0: 0.773040735004023\n",
      "Test ALL Pearson1: 0.8389515026553402 Test  ALL Spearman1: 0.8009204442080389\n",
      "Test ALL Pearson2: 0.8264447330594034 Test  ALL Spearman2: 0.7977902321993732\n",
      "Weight: 0.005 Split: 2 End! PLCC: [0.826296189849452, 0.8023310964708195, 0.8239136024984074] SRCC: [0.7889432837220798, 0.7616858534082814, 0.7905838038038117]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.06437684515796468\n",
      "Train  ALL Spearman0: 0.06341814900904295\n",
      "Train ALL Pearson1: 0.020726837181676467\n",
      "Train  ALL Spearman1: 0.05825319312620636\n",
      "Train ALL Pearson2: 0.4548968474551445\n",
      "Train  ALL Spearman2: 0.46962585340930646\n",
      "Train ALL Pearson0: 0.9267742999631969\n",
      "Train  ALL Spearman0: 0.9139059032637938\n",
      "Train ALL Pearson1: 0.9068714523985186\n",
      "Train  ALL Spearman1: 0.8951191241472019\n",
      "Train ALL Pearson2: 0.9204995115526002\n",
      "Train  ALL Spearman2: 0.9091050544688367\n",
      "Test ALL Pearson0: 0.7637195720646186 Test  ALL Spearman0: 0.7120954410168373\n",
      "Test ALL Pearson1: 0.831667851871722 Test  ALL Spearman1: 0.7901754532644499\n",
      "Test ALL Pearson2: 0.823026530246103 Test  ALL Spearman2: 0.7835195187916718\n",
      "Weight: 0.01 Split: 0 End! PLCC: [0.8061379847274811] SRCC: [0.7619301376909863]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.0068285398158353045\n",
      "Train  ALL Spearman0: -0.026043602760083096\n",
      "Train ALL Pearson1: 0.16748494097172573\n",
      "Train  ALL Spearman1: 0.18962297454272664\n",
      "Train ALL Pearson2: 0.3178377548839574\n",
      "Train  ALL Spearman2: 0.3266934970022842\n",
      "Train ALL Pearson0: 0.9198142984626096\n",
      "Train  ALL Spearman0: 0.9113854505176234\n",
      "Train ALL Pearson1: 0.8978040431533201\n",
      "Train  ALL Spearman1: 0.8797113965464006\n",
      "Train ALL Pearson2: 0.9234323147938466\n",
      "Train  ALL Spearman2: 0.9149203260483237\n",
      "Test ALL Pearson0: 0.8450112836392939 Test  ALL Spearman0: 0.8183913531869238\n",
      "Test ALL Pearson1: 0.8141267419759972 Test  ALL Spearman1: 0.7783392880467512\n",
      "Test ALL Pearson2: 0.8295989490370376 Test  ALL Spearman2: 0.7997527350012636\n",
      "Weight: 0.01 Split: 1 End! PLCC: [0.8061379847274811, 0.8295789915507763] SRCC: [0.7619301376909863, 0.7988277920783129]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.04799853718893894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  ALL Spearman0: -0.04199551353135607\n",
      "Train ALL Pearson1: -0.14856844961081875\n",
      "Train  ALL Spearman1: -0.13773758430059474\n",
      "Train ALL Pearson2: 0.529980386200177\n",
      "Train  ALL Spearman2: 0.5437636622285715\n",
      "Train ALL Pearson0: 0.924049109891697\n",
      "Train  ALL Spearman0: 0.9119370401537791\n",
      "Train ALL Pearson1: 0.9188870420931561\n",
      "Train  ALL Spearman1: 0.9034068982479698\n",
      "Train ALL Pearson2: 0.9050528479626845\n",
      "Train  ALL Spearman2: 0.9012775220283327\n",
      "Test ALL Pearson0: 0.8134815635668114 Test  ALL Spearman0: 0.7876025388626092\n",
      "Test ALL Pearson1: 0.8298940417395629 Test  ALL Spearman1: 0.7923990569609762\n",
      "Test ALL Pearson2: 0.8191080581560952 Test  ALL Spearman2: 0.7886404202104895\n",
      "Weight: 0.01 Split: 2 End! PLCC: [0.8061379847274811, 0.8295789915507763, 0.8208278878208232] SRCC: [0.7619301376909863, 0.7988277920783129, 0.789547338678025]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.022653033693925136\n",
      "Train  ALL Spearman0: 0.041887580809946846\n",
      "Train ALL Pearson1: 0.04414945384042566\n",
      "Train  ALL Spearman1: 0.035473206394829045\n",
      "Train ALL Pearson2: 0.681994646842779\n",
      "Train  ALL Spearman2: 0.6810262882471945\n",
      "Train ALL Pearson0: 0.9367122331578578\n",
      "Train  ALL Spearman0: 0.9258420152351546\n",
      "Train ALL Pearson1: 0.9086062584520442\n",
      "Train  ALL Spearman1: 0.8935780835665849\n",
      "Train ALL Pearson2: 0.9363157245522251\n",
      "Train  ALL Spearman2: 0.9268227238711105\n",
      "Test ALL Pearson0: 0.7761920987524626 Test  ALL Spearman0: 0.7429600853313174\n",
      "Test ALL Pearson1: 0.8321282517398894 Test  ALL Spearman1: 0.7978689403101654\n",
      "Test ALL Pearson2: 0.8204736828951626 Test  ALL Spearman2: 0.7885089986964006\n",
      "Weight: 0.1 Split: 0 End! PLCC: [0.8095980111291716] SRCC: [0.7764460081126278]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: -0.006712437474271162\n",
      "Train  ALL Spearman0: 0.003143838594538802\n",
      "Train ALL Pearson1: -0.1274424911522594\n",
      "Train  ALL Spearman1: -0.12119185232246792\n",
      "Train ALL Pearson2: 0.5282205885856501\n",
      "Train  ALL Spearman2: 0.5447654913553958\n",
      "Train ALL Pearson0: 0.9343763866627869\n",
      "Train  ALL Spearman0: 0.9252637153121414\n",
      "Train ALL Pearson1: 0.9253639600034788\n",
      "Train  ALL Spearman1: 0.9131404791121341\n",
      "Train ALL Pearson2: 0.9277506978598236\n",
      "Train  ALL Spearman2: 0.9190854346324934\n",
      "Test ALL Pearson0: 0.8214896674875324 Test  ALL Spearman0: 0.7904329010200178\n",
      "Test ALL Pearson1: 0.8114348224498136 Test  ALL Spearman1: 0.7806900904210649\n",
      "Test ALL Pearson2: 0.8108387060482666 Test  ALL Spearman2: 0.7752282180814891\n",
      "Weight: 0.1 Split: 1 End! PLCC: [0.8095980111291716, 0.8145877319952041] SRCC: [0.7764460081126278, 0.7821170698408574]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.14518505665394918\n",
      "Train  ALL Spearman0: 0.16640525263074216\n",
      "Train ALL Pearson1: -0.013046876325657913\n",
      "Train  ALL Spearman1: 0.0039373021774688766\n",
      "Train ALL Pearson2: 0.4829591448355294\n",
      "Train  ALL Spearman2: 0.5617043668334771\n",
      "Train ALL Pearson0: 0.9162267983259205\n",
      "Train  ALL Spearman0: 0.9024954007410069\n",
      "Train ALL Pearson1: 0.9259726503162099\n",
      "Train  ALL Spearman1: 0.9122544474014861\n",
      "Train ALL Pearson2: 0.9235795542251161\n",
      "Train  ALL Spearman2: 0.912040754971887\n",
      "Test ALL Pearson0: 0.8272702817635038 Test  ALL Spearman0: 0.8000716010279029\n",
      "Test ALL Pearson1: 0.8367000721498902 Test  ALL Spearman1: 0.7997580552466821\n",
      "Test ALL Pearson2: 0.8214571204916296 Test  ALL Spearman2: 0.7814379783701503\n",
      "Weight: 0.1 Split: 2 End! PLCC: [0.8095980111291716, 0.8145877319952041, 0.8284758248016746] SRCC: [0.7764460081126278, 0.7821170698408574, 0.7937558782149118]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.016280411204386083\n",
      "Train  ALL Spearman0: 0.010770054830123584\n",
      "Train ALL Pearson1: 0.05995871586147219\n",
      "Train  ALL Spearman1: 0.042966805864132665\n",
      "Train ALL Pearson2: 0.5766525986159631\n",
      "Train  ALL Spearman2: 0.6082376642607851\n",
      "Train ALL Pearson0: 0.9103670702969743\n",
      "Train  ALL Spearman0: 0.9022935345117253\n",
      "Train ALL Pearson1: 0.9169126847444382\n",
      "Train  ALL Spearman1: 0.8995310716547532\n",
      "Train ALL Pearson2: 0.9254503566653334\n",
      "Train  ALL Spearman2: 0.911220958814907\n",
      "Test ALL Pearson0: 0.6663079669468346 Test  ALL Spearman0: 0.6431110390988418\n",
      "Test ALL Pearson1: 0.8317735569249439 Test  ALL Spearman1: 0.7919178642079144\n",
      "Test ALL Pearson2: 0.817400502916263 Test  ALL Spearman2: 0.784825805790925\n",
      "Weight: 1 Split: 0 End! PLCC: [0.7718273422626805] SRCC: [0.739951569699227]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.10821692146150493\n",
      "Train  ALL Spearman0: 0.1305490069280132\n",
      "Train ALL Pearson1: -0.1189547726970492\n",
      "Train  ALL Spearman1: -0.12411151128543152\n",
      "Train ALL Pearson2: 0.5401681637484194\n",
      "Train  ALL Spearman2: 0.571019423054113\n",
      "Train ALL Pearson0: 0.9242692887226677\n",
      "Train  ALL Spearman0: 0.9123405845849537\n",
      "Train ALL Pearson1: 0.9207148315240117\n",
      "Train  ALL Spearman1: 0.8981193666281533\n",
      "Train ALL Pearson2: 0.9254371001825635\n",
      "Train  ALL Spearman2: 0.9068462014963874\n",
      "Test ALL Pearson0: 0.6861743823769598 Test  ALL Spearman0: 0.6596904641218632\n",
      "Test ALL Pearson1: 0.8295950576247695 Test  ALL Spearman1: 0.7974580548403901\n",
      "Test ALL Pearson2: 0.8299576500918157 Test  ALL Spearman2: 0.8056476151670627\n",
      "Weight: 1 Split: 1 End! PLCC: [0.7718273422626805, 0.7819090300311817] SRCC: [0.739951569699227, 0.7542653780431053]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Train ALL Pearson0: 0.13764770395481005\n",
      "Train  ALL Spearman0: 0.13414962913882697\n",
      "Train ALL Pearson1: 0.1995953369398814\n",
      "Train  ALL Spearman1: 0.2194549354113688\n",
      "Train ALL Pearson2: 0.43098289022382963\n",
      "Train  ALL Spearman2: 0.4154787160333827\n",
      "Train ALL Pearson0: 0.9428858098621922\n",
      "Train  ALL Spearman0: 0.9177240257930103\n",
      "Train ALL Pearson1: 0.9373944063600554\n",
      "Train  ALL Spearman1: 0.9173469255613802\n",
      "Train ALL Pearson2: 0.9468684590116927\n",
      "Train  ALL Spearman2: 0.9348533163593232\n",
      "Test ALL Pearson0: 0.8028618857465484 Test  ALL Spearman0: 0.7757245563497924\n",
      "Test ALL Pearson1: 0.8248927390894913 Test  ALL Spearman1: 0.7995772486190185\n",
      "Test ALL Pearson2: 0.802099242297062 Test  ALL Spearman2: 0.7735269220146295\n",
      "Weight: 1 Split: 2 End! PLCC: [0.7718273422626805, 0.7819090300311817, 0.8099512890443673] SRCC: [0.739951569699227, 0.7542653780431053, 0.78294290899448]\n"
     ]
    }
   ],
   "source": [
    "run livew_rt25_third_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'prompt_learner1.ctx', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'text_encoder.ln_final.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'prompt_learner0.ctx', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.ln_post.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'prompt_learner2.ctx', 'text_encoder.transformer.resblocks.11.ln_2.weight'}\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25_ftencoders.py:354: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0 = F.softmax(output_0[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25_ftencoders.py:355: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1 = F.softmax(output_1[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25_ftencoders.py:356: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2 = F.softmax(output_2[:, :2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25_ftencoders.py:293: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_0=F.softmax(output_0[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25_ftencoders.py:294: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_1=F.softmax(output_1[:,:2])\n",
      "F:\\STS\\Prompt_IQA/promptiqa_vote_select2_content_filter_quality_livew_third_step_rt25_ftencoders.py:295: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "output2_2=F.softmax(output_2[:,:2])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Train ALL Pearson0: 0.8968628155174624\n",
      "Train  ALL Spearman0: 0.8832217313435635\n",
      "Train ALL Pearson1: 0.9053010622685189\n",
      "Train  ALL Spearman1: 0.8912629544103782\n",
      "Train ALL Pearson2: 0.9033593905000776\n",
      "Train  ALL Spearman2: 0.8884458577174682\n",
      "Train ALL Pearson0: 0.9900388184165819\n",
      "Train  ALL Spearman0: 0.9872771378203364\n",
      "Train ALL Pearson1: 0.990170803979712\n",
      "Train  ALL Spearman1: 0.9873264323034563\n",
      "Train ALL Pearson2: 0.9901299572605504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  ALL Spearman2: 0.9877508037670256\n",
      "Train ALL Pearson0: 0.9948293961063632\n",
      "Train  ALL Spearman0: 0.9940598153314629\n",
      "Train ALL Pearson1: 0.993865949823474\n",
      "Train  ALL Spearman1: 0.9927407482614442\n",
      "Train ALL Pearson2: 0.9939374442038025\n",
      "Train  ALL Spearman2: 0.9934558293639246\n",
      "Train ALL Pearson0: 0.9939116685002439\n",
      "Train  ALL Spearman0: 0.9927310677120768\n",
      "Train ALL Pearson1: 0.9936915884664245\n",
      "Train  ALL Spearman1: 0.9923415426858053\n",
      "Train ALL Pearson2: 0.9943056911403354\n",
      "Train  ALL Spearman2: 0.9935384927959032\n",
      "Test ALL Pearson0: 0.8528393297054678 Test  ALL Spearman0: 0.8057170381426723\n",
      "Test ALL Pearson1: 0.8591281201698157 Test  ALL Spearman1: 0.8188666289111832\n",
      "Test ALL Pearson2: 0.8784923244903384 Test  ALL Spearman2: 0.843219319034272\n",
      "Split: 0 End! PLCC: [0.8634865914552073] SRCC: [0.8226009953627091]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'prompt_learner1.ctx', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'text_encoder.ln_final.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'prompt_learner0.ctx', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.ln_post.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'prompt_learner2.ctx', 'text_encoder.transformer.resblocks.11.ln_2.weight'}\n",
      "Train ALL Pearson0: 0.8975193876184753\n",
      "Train  ALL Spearman0: 0.8826043594507408\n",
      "Train ALL Pearson1: 0.9065598022030206\n",
      "Train  ALL Spearman1: 0.8942787383510362\n",
      "Train ALL Pearson2: 0.906143939284356\n",
      "Train  ALL Spearman2: 0.8897822446752451\n",
      "Train ALL Pearson0: 0.9899445309664467\n",
      "Train  ALL Spearman0: 0.9871037663550845\n",
      "Train ALL Pearson1: 0.9899399041304716\n",
      "Train  ALL Spearman1: 0.9869342407512997\n",
      "Train ALL Pearson2: 0.9896760363519274\n",
      "Train  ALL Spearman2: 0.9871957621779154\n",
      "Test ALL Pearson0: 0.841433876195852 Test  ALL Spearman0: 0.792306854934604\n",
      "Test ALL Pearson1: 0.8511741956454891 Test  ALL Spearman1: 0.8093966179695387\n",
      "Test ALL Pearson2: 0.876866779657468 Test  ALL Spearman2: 0.8432476301204299\n",
      "Split: 1 End! PLCC: [0.8634865914552073, 0.8564916171662698] SRCC: [0.8226009953627091, 0.8149837010081908]\n",
      "Building custom CLIP\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Initializing a generic context\n",
      "Initial context: \"X X X X X X X X\"\n",
      "Number of context words (tokens): 8\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'prompt_learner1.ctx', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'text_encoder.ln_final.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'prompt_learner0.ctx', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.ln_post.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'prompt_learner2.ctx', 'text_encoder.transformer.resblocks.11.ln_2.weight'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALL Pearson0: 0.893777081795705\n",
      "Train  ALL Spearman0: 0.8815384142128656\n",
      "Train ALL Pearson1: 0.9021915493504985\n",
      "Train  ALL Spearman1: 0.888609346081652\n",
      "Train ALL Pearson2: 0.8997055890309497\n",
      "Train  ALL Spearman2: 0.8855291456413432\n",
      "Train ALL Pearson0: 0.9898293180388178\n",
      "Train  ALL Spearman0: 0.986950881275029\n",
      "Train ALL Pearson1: 0.990188590490573\n",
      "Train  ALL Spearman1: 0.9874757194025343\n",
      "Train ALL Pearson2: 0.9902502759014629\n",
      "Train  ALL Spearman2: 0.9879455088013497\n",
      "Test ALL Pearson0: 0.8413334182911354 Test  ALL Spearman0: 0.7923060086167069\n",
      "Test ALL Pearson1: 0.8510226472833514 Test  ALL Spearman1: 0.8092538479593422\n",
      "Test ALL Pearson2: 0.8770182148879817 Test  ALL Spearman2: 0.8434543986049586\n",
      "Split: 2 End! PLCC: [0.8634865914552073, 0.8564916171662698, 0.8564580934874896] SRCC: [0.8226009953627091, 0.8149837010081908, 0.8150047517270026]\n"
     ]
    }
   ],
   "source": [
    "run livew_rt25_fourth_step_ftencoders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
